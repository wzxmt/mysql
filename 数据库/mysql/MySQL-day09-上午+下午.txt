Information_schema获取元数据
Information_schema.tables
数据量: 平均行长度*行数+索引的长度
concat("不变量",变量)   from Information_schema.tables
show
索引及执行计划
Btree , hash  Rtree fulltext  
Btree
聚集索引
辅助索引
唯一索引


alter table t1 add index idx_name(name);
alter table t1 add index idx_n_a(name,age);
alter table t1 add index idx_name(name(10));
alter table t1 drop index idx_name;
desc  
show index from  t1;

explain 
type :ALL  index   range ref  eq_ref  const system null  
key  :被使用的索引
extra:  filesort 


存储引擎

种类:
show engines;
innodb  myisam memory blackhole
tokudb

innodb核心特性:
事务
MVCC
行级锁
热备
CSR
外键

表空间:

共享表空间:ibdata1,系统数据,undo ,tmp
独立表空间: city.ibd  ------> 数据行+索引------>段    区   页(page  16kb)


事务:

ACID

redo:重做日志,记录内存数据页变化,前滚日志,D , AC
undo:回滚日志,记录内存数据页修改之前状态,类似于虚拟机的快照,AC

CSR

锁+隔离级别:I

行级锁:修改行的时候持有这行的排他锁
隔离级别: 
RC   会出现幻读
RR   可以防止幻读(索引锁 GAP  next-lock,了解即可)



日志管理
错误日志
二进制日志
配置
查看分析
截取日志(起点  终点)
set sql_log_bin=0
source a.sql

慢日志
配置
分析慢日志(pt-query-diagest)----->expalin+index----->基于业务的SQL


备份与恢复

备份类型
逻辑:mysqldump
物理:xtrabackup

备份方式
热备
温备
冷备

备份策略
全备 
增量


工具的使用
mysqldump 
-u  -p -h  -S  -P 
-A  -B   world city
-R  --triggers  --master-data=2  --single-transaction

xtrabackup 
全备 
innobackupex   --user=root --password=123 --no-timestamp /backup/full 

--apply-log :redo   undo


增量
innobackupex   --user=root --password=123 --no-timestamp --incremental --incremental-basedir=/backup/full /backup/inc1


basefull: --apply-log --redo-only
inc1 	: --apply-log --redo-only
inc2    : --apply-log --redo-only
inc_Last: --apply-log 
last_full:--apply-log 


主从复制 
主从复制前提
主从复制原理
主从复制故障:SQL , IO ,主从延时
延时从库


==================================
主从复制高级功能


1.半同步复制：

功能：尽可能保证主从数据一致性问题。牺牲主库一定的业务性能,commit
实现过程：
保证IO线程将日志从TCPIP缓存，写入到relaylog才会返回ACK给主库。
会阻塞主库的commit操作，这里会有个超时时间，10秒钟，如果从库还没返回ACK，强制切换为异步复制过程。

1.1 加载插件
主:
INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
从:
INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';
查看是否加载成功:
show plugins;

1.2 启动:
主:
SET GLOBAL rpl_semi_sync_master_enabled = 1;

从:
SET GLOBAL rpl_semi_sync_slave_enabled = 1;

1.3 重启从库上的IO线程
STOP SLAVE IO_THREAD;
START SLAVE IO_THREAD;

1.4 查看是否在运行
主:
show status like 'Rpl_semi_sync_master_status';
从:
show status like 'Rpl_semi_sync_slave_status';
-----------------------------------



2、过滤复制

2.1 主库方面控制
" gjl [world]>show master status ;
+------------------+----------+--------------+------------------+-------------------+
| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+------------------+----------+--------------+------------------+-------------------+
| mysql-bin.000001 |     2691 |              |                  |                   |

Binlog_Do_DB=白名单 ，在此参数中的库，记录二进制日志
Binlog_Ignore_DB=黑名单 ，在此参数中的库，不记录二进制日志

2.2 从库方面控制
Replicate_Do_DB:  白名单，在此参数中的库，复制
Replicate_Ignore_DB: 黑名单，在此参数中的库，不复制

Replicate_Do_Table: 白名单，在此参数中的表，复制
Replicate_Ignore_Table: 黑名单，在此参数中的表，不复制


模糊的表名字
Replicate_Wild_Do_Table: wolrd.t*
Replicate_Wild_Ignore_Table: 


写法：
vim /data/3308/my.cnf
replicate_do_db=world
replicate_do_db=oldboy





环境准备:

0.关闭原有3306数据库,并清理/application/mysql/data下所有数据

pkill mysqld 
rm -rf /application/mysql/data/*


1. 
备份三台机器/etc/my.cnf 文件 
mv /etc/my.cnf /tmp


2.书写配置文件:
master: 10.0.0.51

vim /etc/my.cnf

[mysqld]
basedir=/application/mysql
datadir=/application/mysql/data
socket=/tmp/mysql.sock
log-error=/var/log/mysql.log
log_bin=/data/mysql/mysql-bin
binlog_format=row
skip-name-resolve
server-id=51
gtid-mode=on
enforce-gtid-consistency=true
log-slave-updates=1
[client]
socket=/tmp/mysql.sock


slave1：10.0.0.52

vim /etc/my.cnf
[mysqld]
basedir=/application/mysql
datadir=/application/mysql/data
socket=/tmp/mysql.sock
log-error=/var/log/mysql.log
log_bin=/data/mysql/mysql-bin
binlog_format=row
skip-name-resolve
server-id=52
gtid-mode=on
enforce-gtid-consistency=true
log-slave-updates=1
[client]
socket=/tmp/mysql.sock

slave2：10.0.0.53
vim  /etc/my.cnf
[mysqld]
basedir=/application/mysql
datadir=/application/mysql/data
socket=/tmp/mysql.sock
log-error=/var/log/mysql.log
log_bin=/data/mysql/mysql-bin
binlog_format=row
skip-name-resolve
server-id=53
gtid-mode=on
enforce-gtid-consistency=true
log-slave-updates=1
[client]
socket=/tmp/mysql.sock

3.重新初始化三台机器数据

/application/mysql/scripts/mysql_install_db --basedir=/application/mysql/ --datadir=/application/mysql/data --user=mysql

4. 分别启动三台数据库服务器
/etc/init.d/mysqld start

===============================================

3. GTID


3.1 介绍
GTID(Global Transaction ID)是对于一个已提交事务的编号，并且是一个全局唯一的编号。
它的官方定义如下：

GTID = source_id ：transaction_id

7E11FA47-31CA-19E1-9E56-C43AA21293967:29

3.2 什么是sever_uuid，和Server-id 区别？
source_id 也叫uuid   默认在是第一次启动数据库时，自动生成的

/application/mysql/data/auto.cnf   


手工删除掉此文件，重启数据库，可以生成新的。

3.3 重要参数：
gtid-mode=on
enforce-gtid-consistency=true
log-slave-updates=1

gtid-mode=on						--启用gtid类型，否则就是普通的复制架构
enforce-gtid-consistency=true		--强制GTID的一致性
log-slave-updates=1					--slave更新是否记入日志

3.4 基于GTID的复制构建
master:51
slave:52,53

51:
reset master;
grant replication slave  on *.* to repl@'10.0.0.%' identified by '123';

52\53:
reset master;
change master to master_host='10.0.0.51',master_user='repl',master_password='123' ,MASTER_AUTO_POSITION=1;
start slave;


====
翻车:
                Last_IO_Error: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server ids; these ids must be different for replication to work (or the --replicate-same-server-id option must be used on slave but this does not always make sense; please check the manual before using it).
               Last_SQL_Errno: 0

原因:主库和从库之前的server_id是一样的.

====
Retrieved_Gtid_Set: ddfcc94f-f127-11e8-8210-000c294c83d8:1      接收到的.
Executed_Gtid_Set: ddfcc94f-f127-11e8-8210-000c294c83d8:1       已经执行的.


正常情况下,是和主库中是一样的
show master status;

=============================主从复制基础架构end=========================
主从复制架构演变

1、基本结构
（1）一主一从
（2）一主多从
（3）多级主从
（4）双主
（5）循环复制

2、高级应用架构演变

2.1 高性能架构
（1）读写分离架构(读性能较高)――MySQL proxy(atlas,mysql router,proxySQL(percona),maxscale)、amoeba(taobao)、xx-dbproxy等。
（2）分布式架构(读写性能都提高):分库分表――cobar--->TDDL(头都大了)、Mycat,自主研发等。


2.2 高可用架构
（3）单活:MMM架构――mysql-mmm（google）

（4）单活:MHA架构――mysql-master-ha（日本DeNa）,T-MHA

（5）多活:MGR ――5.7 新特性 MySQL Group replication(5.7.17) --->Innodb Cluster  

（6）多活:MariaDB Galera Cluster架构,Percona XtraDB Cluster、MySQL Cluster架构


============================

0、监控
1、选主
2、数据补偿
3、FAILOVER(故障切换)
4、应用透明(vip)
5. 故障提醒


------------------------

架构介绍:
1主2从，master：db01   slave：db02   db03 ）：
1、MHA 高可用方案软件构成
	Manager软件：选择一个从节点安装
		1、监控问题？ 主机、mysql实例
		2、处理问题，需要人为
		3、数据补偿---->GTID
	Node软件：所有节点都要安装
2、MHA manager的工作过程
（1）Manger要能够监控到所有node节点（通信、MySQL实例状态），需要有一个专门连接node节点的数据库
（2）主库宕机
	（2.1）选择一个新的主库做为代替原主库工作(原则上是选择最接近主库数据状态的从库)
			show slave status\G
			Retrieved_Gtid_Set: 8c49d7ec-7e78-11e8-9638-000c29ca725d:1-3

    （2.2）如果，主库的SSH是可以通信的，从库会立即保存所有缺失的事务
		   mysqlbinlog -------------> /usr/bin/mysqlbinlog  --->ln -s
		   scp 从节点  ------------->各个节点之间建立互信关系
    （2.3）如果SSH不到主库，计算两个从库之间的relay-log的差异，恢复到S2中
（3）S2使用 Change master to S1  重新构建主从
（4）MHA提供了额外的开发接口，和调用方式。
	VIP
	Mail
	binlogserver
（4）这个切换过程完成之后，Manager会立即退出，需要管理员，
		恢复所有节点正常后，再次启动才可以继续工作。

	
		
3、MHA软件构成

Manager工具包主要包括以下几个工具：

masterha_check_ssh 			检查MHA的SSH配置状况 
masterha_check_repl 		检查MySQL复制状况 
masterha_check_status 		检测当前MHA运行状态 
masterha_manger 			启动MHA 
masterha_master_monitor 	检测master是否宕机 
masterha_master_switch 	    控制故障转移（自动或者手动）
masterha_conf_host 			添加或删除配置的server信息


Node工具包（这些工具通常由MHA Manager的脚本触发，无需人为操作）主要包括以下几个工具：
save_binary_logs 			保存和复制master的二进制日志 
apply_diff_relay_logs 		识别差异的中继日志事件并将其差异的事件应用于其他的
slave filter_mysqlbinlog 	去除不必要的ROLLBACK事件（MHA已不再使用这个工具） 
purge_relay_logs 			清除中继日志（不会阻塞SQL线程）



4、 MHA环境搭建
4.1 准备环境（略。1主2从GTID）
4.2 设置从库relay的自动删除功能，从库设置只读



set global relay_log_purge = 0; 	临时（建议三个节点都做）

slave ：db02和db03

set global read_only=1;            


vim /etc/my.cnf

relay_log_purge = 0   				永久，在配置文件，建议在三个节点都做
set global read_only=1; 	 		只在两个从库中设置


4.3、配置关键程序软连接
ln -s /application/mysql/bin/mysqlbinlog /usr/bin/mysqlbinlog
ln -s /application/mysql/bin/mysql /usr/bin/mysql


4.4、配置各节点互信
db01：

rm -rf /root/.ssh 

ssh-keygen

cd /root/.ssh 
mv id_rsa.pub authorized_keys


scp  -r  /root/.ssh  10.0.0.52:/root 

scp  -r  /root/.ssh  10.0.0.53:/root 



----------------------
各节点验证：
db01:
ssh 10.0.0.51 date
ssh 10.0.0.52 date
ssh 10.0.0.53 date

db02:
ssh 10.0.0.51 date
ssh 10.0.0.52 date
ssh 10.0.0.53 date

db03:
ssh 10.0.0.51 date
ssh 10.0.0.52 date
ssh 10.0.0.53 date
----------------------
5、安装软件

（0）
 下载mha软件，mha官网：https://code.google.com/archive/p/mysql-master-ha/
 github下载地址：https://github.com/yoshinorim/mha4mysql-manager/wiki/Downloads

（1）上传MHA.zip到所有节点的/server/tools中，并解压
（2）安装Node软件依赖包
	#安装依赖包
	yum install perl-DBD-MySQL -y
（3）所有节点安装node软件
 rpm -ivh mha4mysql-node-0.57-0.el7.noarch.rpm

6、在db01主库中创建mha需要的用户
 grant all privileges on *.* to mha@'10.0.0.%' identified by 'mha';

7、Manager软件安装（db03）
#安装manager依赖包
yum install -y perl-Config-Tiny epel-release perl-Log-Dispatch perl-Parallel-ForkManager perl-Time-HiRes
#安装manager包
rpm -ivh mha4mysql-manager-0.57-0.el7.noarch.rpm 


8、配置文件准备(db03)

#创建配置文件目录
 mkdir -p /etc/mha
 
#创建日志目录
 mkdir -p /var/log/mha/app1
 
#编辑mha配置文件
 vim /etc/mha/app1.cnf
 
[server default]
manager_log=/var/log/mha/app1/manager        
manager_workdir=/var/log/mha/app1            
master_binlog_dir=/data/mysql/       
user=mha                                   
password=mha                               
ping_interval=2
repl_password=123
repl_user=repl
ssh_user=root                               

[server1]                                   
hostname=10.0.0.51
port=3306                                  

[server2]            
hostname=10.0.0.52
port=3306

[server3]
hostname=10.0.0.53
port=3306


9、状态检查
（1） 互信检查
masterha_check_ssh  --conf=/etc/mha/app1.cnf 

（2）主从状态
masterha_check_repl  --conf=/etc/mha/app1.cnf 


10、开启MHA：
nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null > /var/log/mha/app1/manager.log 2>&1 &


11、查看MHA状态：

 masterha_check_status  --conf=/etc/mha/app1.cnf 


mysql -umha -pmha -h 10.0.0.51 -e "show variables like 'server_id'"
mysql -umha -pmha -h 10.0.0.52 -e "show variables like 'server_id'"
mysql -umha -pmha -h 10.0.0.53 -e "show variables like 'server_id'"
 
 
12、Manager额外参数介绍
 
(1)  ping_interval=1
#设置监控主库，发送ping包的时间间隔，尝试三次没有回应的时候自动进行failover


(2) candidate_master=1

#设置为候选master，如果设置该参数以后，发生主从切换以后将会将此从库提升为主库，
即使这个主库不是集群中事件最新的slave

(3)check_repl_delay=0

#默认情况下如果一个slave落后master 100M的relay logs的话，
MHA将不会选择该slave作为一个新的master，因为对于这个slave的恢复需要花费很长时间，
通过设置check_repl_delay=0,MHA触发切换在选择一个新的master的时候将会忽略复制延时，
这个参数对于设置了candidate_master=1的主机非常有用，因为这个候选主在切换的过程中一定是新的master


13、故障处理
（1）db01:
	停主库：	
/etc/init.d/mysqld stop
	 
观察manager  日志 tail -f /var/log/mha/app1/manager
末尾必须显示successfully，才算正常切换成功。
                                
	
（2）db01:  
启动db01,mysql数据库
/etc/init.d/mysqld start

CHANGE MASTER TO MASTER_HOST='10.0.0.52', MASTER_PORT=3306, MASTER_AUTO_POSITION=1, MASTER_USER='repl', MASTER_PASSWORD='123';	

start slave;
	
(3)db03:

在/etc/mha/app1.cnf加入
[server1]                                   
hostname=10.0.0.51
port=3306  


nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null > /var/log/mha/app1/manager.log 2>&1 &



===================================================

作业:

1.   1主2从,分别复制world和oldboy库
2.   主从复制原理
3.   预习下MHA原理,vip漂移 ,binlog server












+++++++++++++++++++++++++++++++++++++++++++++++++++++
	 如果没有succuessfully，可能会看到的现象：	 

db02：  show slave status\G  
	 还能看到连接主库的信息。
	 db03:show slave \G 
	 有可能会看到连接db02失败
	 /etc/mha/app1.cnf 会发现 server1的配置并没有被清理掉。
	 
	 以上的问题所在可能是，db02从库被手写入了重复事务，导致db03也做了重复事务。
	 
	处理过程：
	 db03 :
		 stop slave;
		 reset slave; 
		 reset master;
	db02:
		stop slave;
		reset slave;
		reset master;
	db03  change master to  db02
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++	 

MHA 的vip功能
14.1 参数：
master_ip_failover_script=/usr/local/bin/master_ip_failover
 
14.2 注意：/usr/local/bin/master_ip_failover，必须事先准备好此脚本

14.3 将script.tar.gz 文件上传到/usr/local/bin，并解压



14.4 修改脚本内容：
vi  /usr/local/bin/master_ip_failover

my $vip = '10.0.0.55/24';
my $key = '1';
my $ssh_start_vip = "/sbin/ifconfig eth0:$key $vip";
my $ssh_stop_vip = "/sbin/ifconfig eth0:$key down";




14.5 更改manager配置文件：
vi /etc/mha/app1.cnf
添加：
master_ip_failover_script=/usr/local/bin/master_ip_failover



14.6 主库上，手工生成第一个vip地址
手工在主库上绑定vip，注意一定要和配置文件中的ethN一致，我的是eth0:1(1是key指定的值)

ifconfig eth0:1 10.0.0.55/24



14.7 重启mha
masterha_stop --conf=/etc/mha/app1.cnf

nohup masterha_manager --conf=/etc/mha/app1.cnf --remove_dead_master_conf --ignore_last_failover < /dev/null > /var/log/mha/app1/manager.log 2>&1 &
















